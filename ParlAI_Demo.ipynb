{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2599487f-0911-4a0e-9b3b-4c1c88e374d4",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://colab.research.google.com/drive/1bRMvN0lGXaTF5fuTidgvlAl-Lb41F7AD#scrollTo=nRJGRtMKmIWV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4af0319-40e6-4400-81d4-3a0c9a375912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "rasa 2.8.10 requires regex<2021.8,>=2020.6, but you have regex 2021.10.23 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q parlai\n",
    "!pip install -q subword_nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9c9a1a-4eeb-4bb0-9391-9a8f1c27796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:09:59 | building data: /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
      "17:09:59 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|█████████████████████████████████████████████████████████| 1.12G/1.12G [01:46<00:00, 10.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:11:59 | \u001b[33mOverriding opt[\"model_file\"] to /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data/models/tutorial_transformer_generator/model (previously: /checkpoint/roller/20190909/cleanreddit/585/model)\u001b[0m\n",
      "17:11:59 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
      "17:11:59 | Using CUDA\n",
      "17:11:59 | loading dictionary from /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data/models/tutorial_transformer_generator/model.dict\n",
      "17:11:59 | num words = 54944\n",
      "17:11:59 | TransformerGenerator: full interactive mode on.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/torch/cuda/__init__.py:143: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:11:59 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
      "17:12:00 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
      "17:12:00 | Loading existing model params from /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data/models/tutorial_transformer_generator/model\n",
      "17:12:00 | Opt:\n",
      "17:12:00 |     activation: gelu\n",
      "17:12:00 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "17:12:00 |     adam_eps: 1e-06\n",
      "17:12:00 |     add_p1_after_newln: False\n",
      "17:12:00 |     aggregate_micro: False\n",
      "17:12:00 |     allow_missing_init_opts: False\n",
      "17:12:00 |     attention_dropout: 0.0\n",
      "17:12:00 |     batch_length_range: 5\n",
      "17:12:00 |     batch_sort_cache_type: pop\n",
      "17:12:00 |     batch_sort_field: text\n",
      "17:12:00 |     batchsize: 48\n",
      "17:12:00 |     beam_block_full_context: False\n",
      "17:12:00 |     beam_block_list_filename: None\n",
      "17:12:00 |     beam_block_ngram: 3\n",
      "17:12:00 |     beam_context_block_ngram: 3\n",
      "17:12:00 |     beam_delay: 30\n",
      "17:12:00 |     beam_length_penalty: 0.65\n",
      "17:12:00 |     beam_min_length: 10\n",
      "17:12:00 |     beam_min_n_best: 3\n",
      "17:12:00 |     beam_size: 8\n",
      "17:12:00 |     betas: '[0.9, 0.98]'\n",
      "17:12:00 |     bpe_add_prefix_space: None\n",
      "17:12:00 |     bpe_debug: False\n",
      "17:12:00 |     bpe_dropout: None\n",
      "17:12:00 |     bpe_merge: None\n",
      "17:12:00 |     bpe_vocab: None\n",
      "17:12:00 |     checkpoint_activations: False\n",
      "17:12:00 |     compute_tokenized_bleu: False\n",
      "17:12:00 |     datapath: /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data\n",
      "17:12:00 |     datatype: train:stream\n",
      "17:12:00 |     delimiter: '\\n'\n",
      "17:12:00 |     dict_build_first: True\n",
      "17:12:00 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "17:12:00 |     dict_endtoken: __end__\n",
      "17:12:00 |     dict_file: /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data/models/tutorial_transformer_generator/model.dict\n",
      "17:12:00 |     dict_include_test: False\n",
      "17:12:00 |     dict_include_valid: False\n",
      "17:12:00 |     dict_initpath: None\n",
      "17:12:00 |     dict_language: english\n",
      "17:12:00 |     dict_loaded: True\n",
      "17:12:00 |     dict_lower: True\n",
      "17:12:00 |     dict_max_ngram_size: -1\n",
      "17:12:00 |     dict_maxexs: -1\n",
      "17:12:00 |     dict_maxtokens: -1\n",
      "17:12:00 |     dict_minfreq: 0\n",
      "17:12:00 |     dict_nulltoken: __null__\n",
      "17:12:00 |     dict_starttoken: __start__\n",
      "17:12:00 |     dict_textfields: text,labels\n",
      "17:12:00 |     dict_tokenizer: bpe\n",
      "17:12:00 |     dict_unktoken: __unk__\n",
      "17:12:00 |     display_add_fields: \n",
      "17:12:00 |     display_examples: False\n",
      "17:12:00 |     display_prettify: False\n",
      "17:12:00 |     distributed_world_size: 64\n",
      "17:12:00 |     download_path: None\n",
      "17:12:00 |     dropout: 0.1\n",
      "17:12:00 |     dynamic_batching: None\n",
      "17:12:00 |     embedding_projection: random\n",
      "17:12:00 |     embedding_size: 512\n",
      "17:12:00 |     embedding_type: random\n",
      "17:12:00 |     embeddings_scale: True\n",
      "17:12:00 |     eval_batchsize: None\n",
      "17:12:00 |     evaltask: None\n",
      "17:12:00 |     ffn_size: 2048\n",
      "17:12:00 |     force_fp16_tokens: True\n",
      "17:12:00 |     fp16: True\n",
      "17:12:00 |     fp16_impl: safe\n",
      "17:12:00 |     gpu: 0\n",
      "17:12:00 |     gradient_clip: 10.0\n",
      "17:12:00 |     hide_labels: False\n",
      "17:12:00 |     history_add_global_end_token: None\n",
      "17:12:00 |     history_reversed: False\n",
      "17:12:00 |     history_size: -1\n",
      "17:12:00 |     image_cropsize: 224\n",
      "17:12:00 |     image_mode: raw\n",
      "17:12:00 |     image_size: 256\n",
      "17:12:00 |     inference: beam\n",
      "17:12:00 |     init_model: None\n",
      "17:12:00 |     init_opt: None\n",
      "17:12:00 |     interactive_mode: True\n",
      "17:12:00 |     interactive_task: True\n",
      "17:12:00 |     invsqrt_lr_decay_gamma: -1\n",
      "17:12:00 |     is_debug: False\n",
      "17:12:00 |     label_truncate: 128\n",
      "17:12:00 |     learn_positional_embeddings: True\n",
      "17:12:00 |     learningrate: 0.0005\n",
      "17:12:00 |     local_human_candidates_file: None\n",
      "17:12:00 |     log_every_n_secs: 30.0\n",
      "17:12:00 |     log_keep_fields: all\n",
      "17:12:00 |     loglevel: info\n",
      "17:12:00 |     lr_scheduler: invsqrt\n",
      "17:12:00 |     lr_scheduler_decay: 0.5\n",
      "17:12:00 |     lr_scheduler_patience: 3\n",
      "17:12:00 |     max_train_time: -1\n",
      "17:12:00 |     metrics: default\n",
      "17:12:00 |     model: transformer/generator\n",
      "17:12:00 |     model_file: /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data/models/tutorial_transformer_generator/model\n",
      "17:12:00 |     model_parallel: False\n",
      "17:12:00 |     momentum: 0\n",
      "17:12:00 |     multitask_weights: [1]\n",
      "17:12:00 |     n_decoder_layers: -1\n",
      "17:12:00 |     n_encoder_layers: -1\n",
      "17:12:00 |     n_heads: 16\n",
      "17:12:00 |     n_layers: 8\n",
      "17:12:00 |     n_positions: 512\n",
      "17:12:00 |     n_segments: 0\n",
      "17:12:00 |     nesterov: True\n",
      "17:12:00 |     no_cuda: False\n",
      "17:12:00 |     num_epochs: 5.0\n",
      "17:12:00 |     numthreads: 1\n",
      "17:12:00 |     numworkers: 4\n",
      "17:12:00 |     nus: [0.7]\n",
      "17:12:00 |     optimizer: fused_adam\n",
      "17:12:00 |     outfile: \n",
      "17:12:00 |     output_scaling: 1.0\n",
      "17:12:00 |     override: \"{'model_file': '/home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/data/models/tutorial_transformer_generator/model'}\"\n",
      "17:12:00 |     parlai_home: /home/calogero/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages\n",
      "17:12:00 |     person_tokens: False\n",
      "17:12:00 |     port: 61337\n",
      "17:12:00 |     pytorch_context_length: -1\n",
      "17:12:00 |     pytorch_datapath: None\n",
      "17:12:00 |     pytorch_include_labels: True\n",
      "17:12:00 |     pytorch_preprocess: False\n",
      "17:12:00 |     pytorch_teacher_batch_sort: False\n",
      "17:12:00 |     pytorch_teacher_dataset: None\n",
      "17:12:00 |     pytorch_teacher_task: None\n",
      "17:12:00 |     rank_candidates: False\n",
      "17:12:00 |     relu_dropout: 0.0\n",
      "17:12:00 |     save_after_valid: True\n",
      "17:12:00 |     save_every_n_secs: -1\n",
      "17:12:00 |     save_format: conversations\n",
      "17:12:00 |     share_word_embeddings: True\n",
      "17:12:00 |     short_final_eval: True\n",
      "17:12:00 |     show_advanced_args: False\n",
      "17:12:00 |     shuffle: False\n",
      "17:12:00 |     single_turn: False\n",
      "17:12:00 |     skip_generation: False\n",
      "17:12:00 |     special_tok_lst: None\n",
      "17:12:00 |     split_lines: False\n",
      "17:12:00 |     starttime: Oct25_17-11\n",
      "17:12:00 |     task: internal:new_reddit:presorted\n",
      "17:12:00 |     temperature: 1.0\n",
      "17:12:00 |     tensorboard_log: False\n",
      "17:12:00 |     text_truncate: 512\n",
      "17:12:00 |     topk: 10\n",
      "17:12:00 |     topp: 0.9\n",
      "17:12:00 |     truncate: -1\n",
      "17:12:00 |     update_freq: 1\n",
      "17:12:00 |     use_reply: label\n",
      "17:12:00 |     validation_cutoff: 1.0\n",
      "17:12:00 |     validation_every_n_epochs: -1\n",
      "17:12:00 |     validation_every_n_secs: 1800.0\n",
      "17:12:00 |     validation_max_exs: 9920\n",
      "17:12:00 |     validation_metric: ppl\n",
      "17:12:00 |     validation_metric_mode: min\n",
      "17:12:00 |     validation_patience: 0\n",
      "17:12:00 |     validation_share_agent: False\n",
      "17:12:00 |     variant: xlm\n",
      "17:12:00 |     verbose: False\n",
      "17:12:00 |     warmup_rate: 0.0001\n",
      "17:12:00 |     warmup_updates: 20000\n",
      "17:12:00 |     weight_decay: 0.01\n",
      "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
      "17:12:00 | creating task(s): interactive\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[0mEnter Your Message:\u001b[0;0m  Hi there!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39697/2373299801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# call it with particular args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m Interactive.main(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# the model_file is a filename path pointing to a particular model dump.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/scripts/interactive.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/scripts/interactive.py\u001b[0m in \u001b[0;36minteractive\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Show some example dialogs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_parleys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# chat was reset with [DONE], [EXIT] or EOF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/tasks/interactive/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0macts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# BatchWorld handles calling self_observe, but we're in a Hogwild or Interactive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0;31m# world, so we need to handle this ourselves.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2143\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_observe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m   2237\u001b[0m                 \u001b[0;31m# save memory and compute by disabling autograd.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0;31m# use `with torch.enable_grad()` to gain back gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2239\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_truncate\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mprefix_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prefix_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             beam_preds_scores, beams = self._generate(\n\u001b[0m\u001b[1;32m    876\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, batch, beam_size, max_ts, prefix_tokens)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataParallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0mencoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/agents/transformer/modules/encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, positions, segments, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# embed input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'xlm'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bart'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp-hands-on-4/lib/python3.8/site-packages/parlai/agents/transformer/modules/encoder.py\u001b[0m in \u001b[0;36mforward_embedding\u001b[0;34m(self, input, positions, segments)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0membedded\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpositions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from parlai.scripts.interactive import Interactive\n",
    "\n",
    "# call it with particular args\n",
    "Interactive.main(\n",
    "    # the model_file is a filename path pointing to a particular model dump.\n",
    "    # Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n",
    "    # They'll be automatically downloaded when you ask to use them.\n",
    "    model_file='zoo:tutorial_transformer_generator/model'\n",
    ")\n",
    "\n",
    "# equivalent to: python -m parlai.scripts.interactive --model-file zoo:tutorial_transformer_generator/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352aead4-c7d5-467b-9eaf-fc3e67186cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The display_data script is used to show the contents of a particular task.\n",
    "# By default, we show the train\n",
    "from parlai.scripts.display_data import DisplayData\n",
    "DisplayData.main(task='empathetic_dialogues', num_examples=5)\n",
    "\n",
    "# we can instead ask to see fewer examples, and get them from the valid set.\n",
    "DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')\n",
    "\n",
    "# python -m parlai.scripts.display_data -t empathetic_dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01900f86-22b0-48a9-8e86-b00a8840a538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee57fd1-44f9-4ee0-8ba5-6a2d96ca36c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
